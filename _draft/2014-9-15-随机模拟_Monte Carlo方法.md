---
layout: post
title: 随机模拟——Monte Carlo方法
comments: true
---

蒙特卡罗方法（Monte Carlo method），也称统计模拟方法，是二十世纪四十年代中期由于科学技术的发展和电子计算机的发明，而被提出的一种以概率统计理论为指导的一类非常重要的数值计算方法。是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。

### 目录
<!-- MarkdownTOC depth=4 -->
- [蒙特卡罗方法的基本思想](#蒙特卡罗方法的基本思想)
- [产生独立样本](#产生独立样本)

- [产生相关样本](#产生相关样本)


<!-- /MarkdownTOC -->

<a name="蒙特卡罗方法的基本思想" />

###蒙特卡罗方法的基本思想
根据[Weikipedia](http://zh.wikipedia.org/wiki/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95)通常蒙特卡罗方法可以粗略地分成两类：一类是所求解的问题本身具有内在的随机性，借助计算机的运算能力可以直接模拟这种随机的过程。另一种类型是所求解问题可以转化为某种随机分布的特征数，比如随机事件出现的概率，或者随机变量的期望值。通过随机抽样的方法，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，并将其作为问题的解。这种方法多用于求解复杂的多维积分问题。

本文主要Monte Carlo 方法来**产生独立样本**和**产生相关样本**

<a name="产生独立样本"/>

###产生独立样本
主要包括基本方法、接受—拒绝（舍选）采样、重要性采样等。


####基本方法
Monte Carlo 方法可以用于模拟、积分、优化等等方面，且在机器学习中，如有隐含变量的非监督学习（对后验概率分布模拟）等中均有应用。这里仅举三个常见的例子。

- **采样与模拟**
利用随机采样，可用近似计算圆周率：让计算机每次随机生成两个0到1之间的数，看以这两个实数为横纵坐标的点是否在单位圆内。生成一系列随机点，统计单位圆内的点数与总点数，（圆面积和正方形面积之比为PI:4，PI为圆周率），当随机点取得越多时，其结果越接近于圆周率（受限于计算机产生的随机数的存储精度）。  
对很多系统，其状态由概率模型控制。假设系统的状态$$x$$， 满足全局约束$$x \in \Omega = \{x:H_i=h_i,i=1,...,K\}$$，这些约束可能一些逻辑约束（如8-皇后问题），也可能是一些宏观性质（如固定体积和能量的物理气体系统），也可能是统计观测（给定一些样例图像，合成有相同性质的图形）。
这里最简单的例子是，给图像随机增加噪声，如增加均值0，方差为3的高斯分布噪声。

- **积分**
非权重蒙特卡罗积分，也称确定性抽样，是对被积函数变量区间进行随机均匀抽样，然后对被抽样点的函数值求平均，从而可以得到函数积分的近似值。此种方法的正确性是基于概率论的[中心极限定理](http://zh.wikipedia.org/wiki/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86)。当抽样点数为m时，使用此种方法所得近似解的统计误差只与m有关（与$$\frac{1}{\sqrt{m}}$$正相关），不随积分维数的改变而改变。因此当积分维度较高时，蒙特卡罗方法相对于其他数值解法更优。

例子：二项分布的贝叶斯推断（回顾贝叶斯理论$$p(\theta)$$（先验分布）, $$p(\theta|X)$$（后验分布）, $$p(X)$$, $$p(X|\theta) $$（似然函数）它们之间的关系可以通过贝叶斯公式进行连接： 后验分布 = 似然函数* 先验分布/ p(X)，此外Beta分布是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数），令$$X ~ Binomial(n, p_1), Y ~ Binomial(m, p_2)$$， 我们要估计$$\sigma = p_1 - p_2 $$， 假设采用扁平先验$$f(p_1, p_2) = f(p_1)f(p_2) = 1 $$，那么后验分布密度为$$f(p_1,p_2|X,Y) ∝ p_1^X(1-p_1)^{n-X} p_2^Y(1-p_2)^{m-Y}$$, 那么$$\sigma$$的后验均值（期望）为$$\hat{\sigma} = \int_0^1 \int_0^1 \sigma(p_1, p_2)f(p_1, p_2|X, Y) dp_1dp_2 = \int_0^1 \int_0^1(p_1 - p_2)p_1^X(1-p_1)^{n-X} p_2^Y(1-p_2)^{m-Y} dp_1dp_2$$，为了避开这些积分计算，接下来我们采用模拟的方法来求解。  
我们注意到$$f(p_1,p_2|X,Y) = f(p_1|X)f(p_2|Y)$$，因此$$p_1, p_2$$在后验分布下是独立的，而且$$p_1|X ~ Beta(X+1, n-X+1), p_2|Y ~ Beta(Y+1, m-Y+1) $$，因此通过抽取$$p_1^i ~ Beta(X+1,n-Y+1), p_2^i ~ Beta(Y+1, m-Y+1)$$, 得到$$\sigma^i = p_1^i - p_2^i$$，抽取次数N之后，可以得到$$\hat{\sigma} = \frac{1}{N} \sum_{i=1} ^N \sigma^i$$。

- **优化**
将一个求最小值的问题转化为求能量最小问题，比如求函数h(x)的最小值，等价于求$$exp{-h(x)/T}$$，可以通过对分布$$\pi(x) ~ exp{-h(x)/T}$$，当T越小时，上述分布在函数h的全局最小值附近的概率越大，当T趋近于0时，从上述分布的样本几乎会在h的全局最小值附近。这就是[模拟退火](http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html)的基本思想。

<a name="接受—拒绝（舍选）采样"/>

####接受—拒绝（舍选）采样
对于简单的模型，我们可以直接采样。但是，对于复杂的模型，需要一些更加复杂的采样方法。接受-拒绝(Acceptance-Rejection Sampling)采样是在给定目标分布密度$$\pi(x)$$，建议密度(Proposal Density) q(x)和常数M，使得对q(x)采样比较容易，q(x)的形状接近$$\pi(x)$$，且对于任意x，有$$\pi(x) \leq Mq(x)$$，从而通过对q(x)的采用来实现对$$\pi(x)$$的采样。

![](http://chrispher.github.com/images/statistics/随机模拟——Monte Carlo方法_接受-拒绝采样.png)

采样过程为：
- 1. 产生样本**X** ～ q(x), 和**U** ～ Uniform[0,1]
- 2. 若$$U \leq \pi(X) / Mq(X)$$，则接受**X**
- 3. 则接受的样本服从分布$$\pi(x)$$

以上等价于：
- 1. 产生样本**X** ～ q(x), 和**U** ～ Uniform[0,1]
- 2. $$Y = Mq(X)U$$，若$$Y \leq \pi(X) $$，则接受**X**

这样的采样效率(接受率，正比于$$\pi(x)$$下的面积除以M与q(x)下面积之积)受M影响。如果q，$$\pi$$是概率密度函数，则$$M = Max{\pi(x) / q(x)}$$。 对于高维为题，M可能很大，此时拒绝率接近100%.


<a name="重要性采样"/>

####重要性采样
重要性采样是通过从已知采样的概率q(x)采样，来近似积分$$I = \int f(x)\pi(x)dx = \intf(x) \frac{\pi(x)}{q(x)q(x)dx}$$（而不是产生新的样本）
![](http://chrispher.github.com/images/statistics/随机模拟——Monte Carlo方法_重要性采样.png)

流程如下：
- 1. 产生样本$$X_1, ... ,X_N ~ q(x)$$
- 2. 计算 $$\hat{I} = \frac{1}{N} \sum_{i=1}^N f(X_i)w(X_i)$$, 其中$$ w(X_i) = \frac{\pi(X_i)}{q(X_i)}$$

该估计是一致估计，即$$\frac{1}{N} \sum_{i=1}^N f(X_i)w(X_i) \longrightarrow ^p \intf(x)\pi(x)dx $$



<a name="产生相关样本"/>

###产生相关样本
之前提到的采样方法都是产生独立样本，而且重要性采样和接受-拒绝采样都是只在q(x)和$$\pi(x)$$很相似时才表现很好。而且，在高维空间问题中，标准的采样方法会失败（接受-拒绝采样中，拒绝率接近100%；重要性采样中，样本权重趋近于0）。对高维复杂问题，用马尔科夫链（Markov Chain） 产生一些列相关样本，实现对分布的采样。

<a name="Markov Chain Monte Carlo"/>

####Markov Chain Monte Carlo
MCMC：一种利用一定范围内的均匀分布的随机数，对高维空间概率进行采样的通用技术。基本思想是设计一个马尔科夫链，使得其稳定概率为目标
分布$$\pi(x)$$.

<a name="Metropolis-Hastings算法"/>

####Metropolis-Hastings算法

<a name="Gibbs Sampler"/>

####Gibbs Sampler