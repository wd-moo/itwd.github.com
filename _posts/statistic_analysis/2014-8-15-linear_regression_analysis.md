---
layout: post
title: 应用线性回归分析
category: 统计分析
tags: [统计学]
description: 简述线性回归问题，目的在厘清回归问题中涉及的基本假设、检验方法等，除了必要的公式和计算，着重于如何做好回归分析
---

本文主要简述线性回归问题，目的在厘清回归问题中涉及的基本假设、检验方法等，除了必要的公式和计算，本文将着重于如何做好回归分析。**笔记主要参考《An Introduction to Statistical Learning with Applications in R》、《完全统计学教程》、维基百科等资料**。

<!-- more -->

###目录
{:.no_toc}

* 目录
{:toc}


###1.线性回归概述
回归是研究因变量$$Y$$和自变量$$X$$关系的方法。自变量也称为预测变量、协变量或特征。总结 $$X$$ 和 $$Y$$ 的关系的一种方法是通过回归函数: $$r(X) = E(Y|X=x) = \int yf(y|x)dy$$, 目标是用形如 $$(X_1,Y_1),\ldots,(X_n,Y_n) \sim F_{XY}$$ 的数据估计回归函数$$r(x)$$ .
给定一个随机样本 $$(Y_i, X_{i1}, \ldots, X_{ip}), \, i = 1, \ldots, n $$, 一个多变量线性回归模型表示为:$$ Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + \varepsilon_i, \qquad i = 1, \ldots, n $$.   
为了估计这些参数，我们使用矩阵表示: $$ Y = X \beta + \varepsilon \,$$ (注意区分随机变量、参数和观测值).  使用矩阵(或[最小二乘法](http://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95))得到参数估计 $$\hat\beta = (X^T X)^{-1}X^T y$$ . 


###2.基本过程

####2.1提一些问题
1.自协变量(预测变量)与因变量之间有什么关系吗，这种关系有多强烈呢？  
2.哪些自变量对相应变量有影响，能否评估这种影响大小呢？  
3.模型拟合的效果如何评价？  
4.如何模型拟合效果不佳，我们该怎么办？

####2.2古典假设
1.样本是在母体之中随机抽取出来的。  
2.因变量Y在实直线上是连续的。  
3.残差项是独立且相同分布的(iid)，即残差是独立随机的，且服从均值为0，方差为$$\sigma$$高斯分布。


####2.3回归推论
1.对于每一个 $$i=1,\ldots,n$$ ，我们用 $$\sigma^2$$ 代表误差项 $$\varepsilon$$ 的方差（即标准误）。对于参数个数为 p + 1 的模型，标准误的一个无偏估计是：$$\hat \sigma^2  = \frac {RSS} {n-p} $$，其中 RSS 是误差平方和（残差平方和, residual-sum square).
估计值和实际值之间的关系是：$$\hat\sigma^2 \cdot \frac{n-p}{\sigma^2} \sim  \chi_{n-p}^2 $$， 其中 $$\chi_{n-p}^2 $$服从卡方分布，自由度是n-p。
2.对普通方程的解可以写为：$$\hat{\boldsymbol\beta}=(\mathbf{X^TX)^{-1}X^Ty}$$.这表示估计项是因变量的线性组合。进一步地说，如果所观察的误差服从正态分布。参数的估计值将服从联合正态分布。在当前的假设之下，估计的参数向量是精确分布的。$$\hat\beta \sim N ( \beta, \sigma^2 (X^TX)^{-1} )$$，其中$$N(\cdot)$$表示多变量正态分布。
3.参数估计值的标准差 $$\hat\sigma_j=diag(\sqrt{ \frac{RSS}{n-p}[\mathbf{(X^TX)}^{-1}]})$$ 参数$$\beta_j$$ 的$$100(1-\alpha)\%$$ 置信区间可以用以下式子来计算：
$$\hat \beta_j  \pm t_{\frac{\alpha }{2},n - p} \hat \sigma_j $$ (因为均值为0). 
误差项可以表示为：$$ \mathbf{\hat r =  y-X \hat{\boldsymbol{\beta}}= y-X(X^TX)^{-1}X^Ty}$$

####2.4假设检验
1.是否存在关系  
标准误$$\sigma^2$$也可以用于系数的假设检验。这里为了简化问题，我们以$$Y = \beta_0 + \beta_1 X + \varepsilon$$我们的零假设：$$H_0$$: $$X$$和$$Y$$之间没有关系，备择假设就是两者之间存在某些关系。用数学表示，零假设就是$$H_0: \beta_1 = 0$$, 备择假设为$$H_1: \beta_1 \neq 0$$. 实际中，我们通常采用t检验，$$t = \frac{\hat{\beta_1}-0}{\hat{SE(\beta_1)}}$$, 这里SE表示标准误。  
2.模型是否准确  
判断回归模型质量好坏主要是：标准化残差($$RSE$$)和
$$R^2$$统计量。为了方便我们的进一步讨论，回想以下上面的表示：  
$$RSS = \sum_{i=1}^n (y_i - \hat{y_i})^2$$  
$$RSE = \sqrt{\frac{RSS}{n-p}} = \hat \sigma$$  
$$TSS = \sum_{i=1}^n (y_i - \hat(y))^2$$  
$$R^2 = 1 - \frac{RSS}{TSS}$$  
$$RSE$$能够提供直接的拟合效果测量，但是我们仍不好判断究竟多少的$$RSE$$是好的。$$R^2$$提供了一种测量，它是一种方差解释率，即预测值能解释实际值波动的比例，换句话说就是$$Y$$值波动性能被用$$X$$解释的比例。$$R^2$$的值介于0-1之间，越接近于1，说明拟合效果越好。此外，对于两个变量而言，$$R^2$$通常等于我们所说的**相关系数**。  
3.$$F$$检验  
对于多元回归分析，我们的零假设会调整为$$H_0:\beta_1=\beta_2=\ldots=\beta_p=0$$,(即对整个模型做检验)备择假设是至少有一个$$\beta_j$$是非零的。这个时候，我们采用$$F$$检验，其中$$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$

###3.误差分析
在回归分析中，存在很多潜在问题。当然，我们的假设是古典假设，细化起来的话，有一些假设([参考维基百科](http://en.wikipedia.org/wiki/Linear_regression))，主要是自变量不随机、线性关系、残差方差固定、误差独立、不存在多重共线性。

####3.1非线性
线性回归模型假设了因变量和自变量之间的关系是线性的，而它们之间的实际关系很少是线性的。因此，我们得到的模型是有待进一步考证的，模型预测的准确率也会因非线性而大大折扣。残差图(Residual plots) 是判断非线性的一个有用的工具。  
考虑到自变量个数可能多余一个，因此我们绘制的残差图是以预测值为横轴，残差值为纵轴。如下图所示：  

<img src="/images/statistics/应用回归分析-非线性.jpg" height="100%" width="100%">
上图左侧图中，残差存在一定的模式，增加二次项能够提高拟合效果。如果残差图显示没有非线性关系存在，那么直接使用线性回归就可以。如果发现残差图中存在某些模式，那么可以对原始数据进行变换，比如$$logX, \sqrt{X}, X^2$$等。

####3.2误差项相关
在线性回归中，一个重要的假设是残差项$$\varepsilon_1,...,\varepsilon_n$$是不相关的。在回归模型的参数估计中，使用的标准误是基于残差项相互独立的，而实际中，残差项之间可能存在相关关系，从而使得标准误的估计有误，即使得标准误被**低估**了，从而置信区间和预测区间都会被缩窄，此外P值也会变小(参考维基百科：标准误-[样本相关性校正](http://zh.wikipedia.org/wiki/%E6%A0%87%E5%87%86%E8%AF%AF))。  
在时间序列中，因为惯性等各种因素，经常出现**自相关**([参考资料:自相关](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0CDIQFjAB&url=%68%74%74%70%3a%2f%2f%65%63%6f%2e%68%75%73%74%2e%65%64%75%2e%63%6e%2f%6a%69%6c%69%61%6e%67%2f%45%63%6f%6e%6f%6d%65%74%72%69%63%73%2f%64%6f%77%6e%6c%6f%61%64%2f%25%45%37%25%41%43%25%41%43%39%25%45%37%25%41%42%25%41%30%25%45%46%25%42%43%25%38%38%25%45%38%25%38%37%25%41%41%25%45%37%25%39%42%25%42%38%25%45%35%25%38%35%25%42%33%25%45%46%25%42%43%25%38%39%2e%70%70%74&ei=o8ofU8fkC4qZiAeZ8oH4Dg&usg=AFQjCNHmVLiydcZ-G6IQIz_yb_dWRbmEMg&sig2=-xXIYwSyh_Cm4NGqNbT11A&bvm=bv.62788935,d.aGc))。绘制序列-残差图，可以发现残差之间的自相关。这里就不再上图了，在很多时间序列分析中应该会有详细的谈及。在非时间序列问题中，也可能存在自相关。比如在研究人的身高跟体重的关系时，因为没考虑两个人是否来自同一个家庭或者饮食、成长环境是否相似等因素，导致自相关的存在。  
注意：残差项相互独立不仅仅是线性回归分析中，也是其他统计模型中非常重要的假设，好的**试验设计**能够降低自相关的存在风险。

####3.3误差方差不固定
在线性回归分析中，关于残差的另一个重要假设是$$Var(\varepsilon_i) = \sigma^2$$, 标准误、置信区间以及线性模型的假设检验均基于该假设。然而，残差项方差不固定却是很常见。如下图左侧图所示，可能方差会随着观测值的增加而增加。  

<img src="/images/statistics/应用回归分析-误差方差不固定.jpg" height="100%" width="100%">
对于这样的问题，一般采取的方法是对因变量$$Y$$进行凹函数变换处理，如$$logY, \sqrt{Y}$$,结果如上图右侧图所示: 残差项的方差基本上固定，虽然看起来有一点非线性项的存在。（这里的残差图仍然是预测值y和残差）。  
另外一个处理方式是采用**加权最小二乘法**(weighted least squares), 对于上面的例子而言，可以令$$w_i = n_i$$，这样对应的方差可以调整为$$\sigma_i^2 = \sigma^2 / n_i$$. 

####3.4异常值
异常值指的是$$y_i$$非常远离模型的预测值(残差较大)。异常值，可能是很多原因引起的，比如记录错误等。异常值会引起标准误增加，使得$$R^2$$降低。残差图可以用来判断是否存在异常值。如下图所示：  

<img src="/images/statistics/应用回归分析-异常值.jpg" height="100%" width="100%">
但是，实际中我们很难判断多大的残差才算是异常值呢？为此，我们通常绘制**学生残差**([studentized residuals](http://en.wikipedia.org/wiki/Studentized_residual),  简单的说，误差均值为0，标准残差=残差/样本残差的标准差， 而学生残差=残差/残差标准差的点估计值)，学生残差能够更精确的反映误差的方差在每个点上的实际差异。处理异常值的常用方法是剔除，但是需要关注一下这个值异常的原因，异常值也可能是模型的缺陷。  

####3.5高杠杆点
与给定 $$x_i$$ 下观察 $$y_i$$ 是否是异常值对应，如果样本有一个异常的 $$x_i$$ ,称之为高杠杆点。如下图所示

<img src="/images/statistics/应用回归分析-高杠杆点.jpg" height="100%" width="100%">
左图，红线拟合了全部数据，而蓝线拟合了去除高杠杆点后的。高杠杆点在最小二乘法中，对于估计具有尺度的杠杆效应，使得最小二乘法受到该点的影响非常大。在单变量情况下，我们可以绘制散点图，但是多变量情况下，就无法绘制了。中间图说明了，我们绘制两个自变量的散点图，能够发现高杠杆点的情况，然而该点的$$X_1$$单独对于$$X_1$$却不是高杠杆点。所以我们需要考虑的是多有的自变量，而不是单一的。  
通常情况下，我们会计算**杠杆统计量**([leverage statistic](http://en.wikipedia.org/wiki/Leverage_(statistics))), 对于简单线性回归，$$h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i^\prime=1}^n(x_{i^\prime} - \bar{x})^2}$$. $$h_i$$是介于1/n到1之间。对于p个自变量，所有样本的杠杆统计量均值为 $$(p+1)/n$$, 如果杠杆统计量大于均值，我们可以怀疑对应的点是否是高杠杆点。在R语言中，绘制杠杆-残差图中，会标识出这些高杠杆点。

####3.6共线性
共线性([collinearity](http://en.wikipedia.org/wiki/Collinearity))是指自变量中两个或多个之间存在较强的相关关系。共线性在线性回归分析中，使得我们很难分析单个自变量对因变量的影响。此外，共线性也使得参数估计准确性下降，标准误估计增加。  
一个判断是否有共线性的方法是看自变量的相关矩阵([correlation matrix](http://en.wikipedia.org/wiki/Correlation_matrix#Correlation_matrices)), 然而相关矩阵能很好的判断两两变量之间的相关性，但是不能判断超多两个变量间的相关关系(这种情况成为**多重共线性**(multicollinearity))。解决多重共线性的方法是计算**方差膨胀因子**(VIF, [variance inflation factor](http://en.wikipedia.org/wiki/Variance_inflation_factor)), VIF是使用所有变量拟合模型的$$\hat{\beta_j}$$的方差，除以只用该变量拟合得到的参数的方差。VIF最小是1，但在实际应用中，允许存在一定的共线性，一般VIF大于5或10，则表明存在共线性。对于每一个变量的VIF计算公式, 为$$VIF(\hat{\beta_j}) = 1 / (1 - R^2_{X_j|X_{-j}})$$, 这里$$R^2_{X_j|X_{-j}}$$ 指以$$X_j$$为因变量，其他变量为自变量拟合得到模型的$$R^2$$。如果$$R^2_{X_j|X_{-j}}$$接近1，说明该变量可以用其他变量的线性组合得到，VIF值越大。  
解决共线性通常有两个方法，一是剔除某个有问题的变量，这对模型通常没有太大坏影响。另外一个方法是把这些共线性变量整合成一个新变量。

###4.附录
**置信区间估计**(confidence interval estimate)：利用估计的回归方程，对于自变量 $$x$$ 的一个给定值 $$x_0$$ ，求出因变量 y 的平均值的估计区间。

**预测区间估计**(prediction interval estimate)：利用估计的回归方程，对于自变量 $$x$$ 的一个给定值 $$x_0$$ ，求出因变量 y 的一个个别值的估计区间。


