---
layout: post
title: Structure of a Data Analysis
category: 统计分析
tags: [数据科学]
description: 介绍了数据分析的基本流程和文件存放方法等等
---

随着工作和学习不断进行，越发认识到一个好的框架能够让你更快更容易的着手分析，当然也会有思维定势的缺陷。然而，作为基本流程，确实必须的，该数据分析流程能够保证在数据分析过程不遗落分析步骤，免得出现比如忘记清理异常值等情况。有些咨询公司甚至把适合自己公司的数据分析流程申请专利，可见分析流程的价值，而标准商业数据挖掘流程(CRISP)也由企业层面上升到了行业标准,在后面会整理到。  
本文根据coursera的数据分析课程——由霍普金斯大学统计生物学助理教授Jeffrey Leek 的课件和视频整理完成。 

<!-- more -->

###目录
{:.no_toc}

* 目录
{:toc}

###1.概述
数据分析的基本流程主要包括以下11个流程：  
- 定义一个问题(Define the question)
- 定义一个理想的数据集(Define the ideal data set)
- 确定哪些数据可以获得(Determine what data you can accsess)
- 获取数据(Obtain the data)
- 清洗数据(clean the data)
- 探索性数据分析(Exploratory data analysis)
- 统计预测/模型(Statistical prediction/modeling)
- 结果解释(Interpret results)
- 拓展结果(Challenge results)
- 撰写报告(Synthesize)
- 创建可重复代码(Create reproducible code)  
接下来，我们具体谈一谈。

###2.提问与获取数据
- ####定义问题
仅仅是数据分析，在其他的一些科学中，“提出一个有价值的问题”比“寻找一个合适的答案”要难得的多。而数据分析中，更是这样(涉及太多业务)。问问我们自己，在不同的情况下，什么问题是我们已经解决的，什么问题是有解决的价值呢？比如在给定了很多数据的情况下，或者在信息不对称的情况等。这里提一句，数据分析并不是非常强调精美的模型，也不是可视化开发，而是将实际问题转化为用数据求解的科学方法。  
举个例子而言，我们最开始的问题是“我能否自动的判断一封邮件是否是垃圾邮件？”，进一步把问题转化成数据分析问题“我能否通过邮件的一些**定量的特征**把一封邮件**分类**为垃圾邮件和非垃圾邮件？”。定义问题，就需要把原来很泛化的问题转化成数据分析问题。

- ####定义数据集
数据集是根据我们的目标而定。如果是为了完整的描述，那么我们可能需要全部的数据；如果是为了探索，那么抽样数据就可以；如果是做推断，那么需要适合的样本和总体；如果是为了预测，就需要来自同一总体的训练集和测试集。此外，我们的目的还可能涉及业余研究、系统分析等等，不同的目的对数据的要求也不一样。

- ####获取数据
首先我们需要确定我们可以获得哪些数据。比如我们可以从企业获得、有时候可以从网上获取、从某些实验室获得，甚至是购买获得。**在数据使用的时候，一定要注意遵守使用条款**，而在企业内部，请注意权责分明。如果数据不存在的话，那么需要我们自己去通过实验、调研等获得。  
之后，我们就可以尝试获取数据。在写作的时候，一定要标注这些数据来在什么地方（有时候，来自企业自身的数据库，最好标明）。此外，如果我们的数据来自互联网，一定要注意把网址记录下来，尤其是数据下载的**日期**。
针对刚才的例子，垃圾邮件分类，我们的目标是预测一封邮件的分类。需要的数据是许多邮件且标注了是否是垃圾邮件。而数据集可以从[UCI](http://archive.ics.uci.edu/ml/datasets/Spambase)上获得。

###3.清洗数据和探索
最开始获得的数据，通常都需要进行清洗。在清洗之前，我们要确认数据是否已经清洗，如果清洗的话，需要注意他们是如何清洗的。此外，我们需要非常关注数据源（数据的产生方式、完整性、是否抽样等等）。很多数据都是需要我们进一步的**规范化**、**抽样处理**等。在数据清洗的时候，需要确认数据是否足够有用、是否需要更换数据。  
通常情况下，数据清洗是和数据探索是在一起的。整个过程也是最耗费时间的，通过数据探索了解数据的基本情况、数据类型、缺失情况、重复值等等，之后针对不合理的情况进行清洗和规整。在数据清洗和探索过程中，需要关注**数据类型**、**重复记录**、**错误记录**、**异常情况**、**缺失值**等。清洗规则，需要根据我们**目标**和**业务**而调整的。  
此外，在数据探索过程中，我们通常都会绘制一些图表（如箱型图、散点图、直方图等）、做列连表等形式方便我们更好的认识数据、认识数据的分布。有些时候，我们也会使用聚类等方法以求更好的认识数据。注意：**数据分布**的情况是非常重要，这是概率分布基础。很多时候，模型结论的错误都是由样本分布有偏导致的（比如数据中男女分布比例为10：1，而实际可能是2：1）。  

###4.统计模型与解释
我们的模型应该是在探索分析的基础之上。而使用的方法则是由我们的问题和目标而定的。这里需要特别注意的是：在分析过程中，会涉及到**预处理**。与清洗不同，数据预处理强调数据转换和变换，比如数据的归一化、离散化，文本矩阵化。模型建立过程中需要注意**不确定性**的衡量。  
在得到我们的模型或预测结果之后，我们需要解释这些结果。在解释结果的时候，需要注意我们的用语，比如相关(不是因果)、显著关系、置信区间等等。此外，需要解释模型中的参数，比如回归分析中系数表示什么。通常情况下，我们都会解释结果的**不确定性**，比如选择p值、置信度等。  
对于垃圾邮件，我们的结果可以解释为：含有dollar的邮件，是垃圾邮件的概率是多少；模型误差率是多少等等。

###5.拓展结果与撰写报告
对于我们的结果，过程存在很多挑战，比如问题的变化、数据源可靠性、预处理合理性、分析和结论是否可拓展等等。我们需要指出过程中的各种不确定性(一般指出相对重要的就可以)。比如数据来自美国的就业人口数据，那么可能对中国就不适合（很多数据都具有这种潜在的适用性问题）。  
我们可以讨论分析中的各种潜在问题，抛出一些可进一步分析和讨论的东西。  
在撰写报告时，我们以提出问题开始，之后简要的概述结论(类似于讲故事的形式)。在报告中，并不需要包括所有的分析过程，除非他对我们的分析很重要，或者对以后拓展有价值。报告以讲故事的形式呈现，比按照时间去写要好很多（清晰明了）。注意：在报告中，需要选择合适的图片。

###6.代码与备份
在报告完成后，我们需要规整文件，用于备份和下次使用。对于代码，我们可以再整理，用于系统的部署等，也可以提交给他人，用于研究学习等。  
这里，我们简单的提及一下数据分析中比较重要的一点：**文件夹存放和文件命名**。一般而言，用项目名来命名文件夹，其下分为如下几个文件夹：一个文件夹raw，用于存储原始数据（原始必须备份，不能破坏）；一个文件夹data，用于存放处理的数据；一个文件夹code，用于存放分析程序；一个文件夹是Writing，用于存放分析报告；有的也会有一个文件夹figures，用于存放图表(可以是程序输出的数据，用Excel作图等)。最后可能会一个文件夹final,用于整理提交给别人或者备份等。当然，也可以增加一个文件夹reference，用于存放参考资料等。当然，这些都是参考，在实际工作中，需要根据需要明确文件夹分类，以便查阅、备份等等。  
此外，文件的命名，一般是项目名或者其他有意义的名字。对于程序或者报告，我们需要注意增加编辑人和版本，通常我们也会增加编辑日期，比如最终的报告名字为：互联网分析报告\_张三\_20140207\_v3.doc。当然，这些都是仅供参考，大家可以根据企业的规定或者个人习惯而定，但文件命名主要是为了可追溯——责任追溯（你懂的）、版本追溯，以免混淆版本，发送了未完成的报告或者之前的报告。